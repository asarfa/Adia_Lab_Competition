{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd1A1tFBOKs"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/crunchdao/adialab-notebooks/blob/main/basic_submission.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWkyd7l6wBse",
        "tags": []
      },
      "source": [
        "# ![title](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGffwE2wBsi"
      },
      "source": [
        "# Set up your crunch workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPwpdl83wBsi"
      },
      "source": [
        "#### STEP 1\n",
        "Run this cell to install the crunch library in your workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fctOAv3CwBsi",
        "outputId": "351cf026-35ba-440c-d751-08d12db2042f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip3 install crunch-cli --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaVIsoyowBsl"
      },
      "source": [
        "#### STEP 2\n",
        "Import the crunch package and instantiate it to be able to access its functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHV_tK7EwBsm",
        "outputId": "fec2a69c-b2d9-4d97-e003-82749dc7cef5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "crunch = crunch.load_notebook(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhDWzOWDBOKw"
      },
      "source": [
        "#### STEP 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C7QjMkqBOKw",
        "outputId": "22b538b9-9937-4fea-f87e-1f0505eb559a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# go to your submit page and copy paste your setup command to access the data\n",
        "# https://adialab.crunchdao.com/submit\n",
        "#!crunch --notebook setup annual-alban --token 5dbzDSvzqQ1x77VS7EMptZAsp4PrtiBUh9m8vPwhgt6Wl2QRqC7Z0rJXT6Bjw7pc\n",
        "#%cd annual-alban"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJqTMeLqwBsm"
      },
      "source": [
        "# ADIA Lab Market Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlAzMPaIwBsm"
      },
      "source": [
        "## A code competition\n",
        "\n",
        "This competition is divided in two phases.\n",
        "\n",
        "Submission phase - 12 weeks\n",
        "\n",
        "Out-of-Sample phase - 12 weeks\n",
        "\n",
        "During the first phase, participants will submit Python notebooks or Python scripts which build their best possible model on the data proposed by the organizers. In the second phase, also called [Out-of-Sample](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) (OOS) phase, the participant's code will be automatically executed by the platform on unseen data. During this phase, the participants won't be able to modify their code.\n",
        "\n",
        "There are two main reasons for this:\n",
        "\n",
        "- Participants won't be able to game or cheat their scores.\n",
        "\n",
        "- [Overfitting](https://deliverypdf.ssrn.com/delivery.php?ID=634087103098022017102089127026118070055022030067038035066070070118003108076075122073107013020035005031116084117030102014013119017036066065011126115081078006004108029033051020066006092025091103065117104075029100098011096065096065079019015002101078070&EXT=pdf&INDEX=TRUE) of the training data will lead to very bad performance OOS.\n",
        "\n",
        "To ensure reproducibility of your work, you will need to follow certain coding guidelines to participate in the competition. These guidelines will also allow our scoring system to run your code in the cloud during the OOS period without any issues.\n",
        "\n",
        "In this competition, CrunchDAO acts as a third party intermediary and will never communicate this code to the organizers.\n",
        "\n",
        "## The Coding Guideline\n",
        "\n",
        "Your submission needs at least to provide three components: imports, `train()`, and `infer()`.\n",
        "\n",
        "1. **imports**: As with any script, if your solution contains dependencies on external packages make sure to import them. The system will automatically install your dependancies. Make sure that you are using only packages that are whitelisted [here](https://adialab.crunchdao.io/submit?tab=libraries).\n",
        "\n",
        "2. **`train()`**: In the training phase the users will build the model and train it such that it can perform inferences on the testing data. The model must be saved in the `resources` directory.\n",
        "\n",
        "4. **`infer()`**: In the inference function the model trained in the previous step will be loaded and used to perform inferences on a data sample matching the characteristic of the training test.\n",
        "\n",
        "## Scoring on the public leaderboard\n",
        "\n",
        "To ensure the reliability of the public leaderboard, you don't have access to all of the testing data on which you will be scored.\n",
        "The `X_test` data downloaded in your workspace is composed of **only** 5 dates to test locally that your code actually runs when submitted. Notice that `y_test` is not available for those 5 dates.\n",
        "Once you have uploaded your solution the system will run your code on a larger test set of approximately 30 dates, and compute the score.\n",
        "\n",
        "When submitting, you are left to decide how frequently to retrain your model, considering the limit of 5 hours of resources / week / user allowed to predict the 30 dates of the private test set.\n",
        "\n",
        "\n",
        "## Scoring on the out-of-sample phase\n",
        "\n",
        "During the out-of-sample phase, after the submission period ends, the system will call your code 3 times every week on live datapoint.\n",
        "\n",
        "The mean spearman score after 12 weeks of OOS will determine the winners of the tournament.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwxiUFGAwBsq"
      },
      "source": [
        "# Construction of a basic submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o1AvFK_wBso"
      },
      "source": [
        "### Submission process\n",
        "\n",
        "1- Make sure to put all your code in the code interface inside your Notebook. The system will parse these functions to execute it in the cloud. You can work outside of the code interface but to be able to submit you will need to fill in the submission function with the code you want to submit\n",
        "\n",
        "2- Once satisfied with your work. Download this notebook ( file -> Download -> Download.ipynb )\n",
        "\n",
        "3- Then upload this Notebook on https://adialab.crunchdao.com/submit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "from typing import Dict, List, Tuple, Union\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta\n",
        "from scipy.optimize import fsolve\n",
        "from pandas import to_datetime\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import pickle\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchmetrics import RetrievalNormalizedDCG, SpearmanCorrCoef\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, seed_everything\n",
        "from attr import attrib, attrs\n",
        "import crunch\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # Ignore Setting With Copy Warning\n",
        "\n",
        "PADDED_Y_VALUE = -2\n",
        "PADDED_INDEX_VALUE = -2\n",
        "\n",
        "\n",
        "def first_arg_id(x, *y):\n",
        "    return x\n",
        "\n",
        "\n",
        "class FCModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents a fully connected neural network model with given layer sizes and activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sizes, dropout, n_features):\n",
        "        \"\"\"\n",
        "        :param sizes: list of layer sizes (excluding the input layer size which is given by n_features parameter)\n",
        "        :param input_norm: flag indicating whether to perform layer normalization on the input\n",
        "        :param activation: name of the PyTorch activation function, e.g. Sigmoid or Tanh\n",
        "        :param dropout: dropout probability\n",
        "        :param n_features: number of input features\n",
        "        \"\"\"\n",
        "        super(FCModel, self).__init__()\n",
        "        sizes.insert(0, n_features)\n",
        "        layers = [nn.Linear(size_in, size_out) for size_in, size_out in zip(sizes[:-1], sizes[1:])]\n",
        "        self.input_norm = nn.Identity()\n",
        "        self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout or 0.0)\n",
        "        self.output_size = sizes[-1]\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the FCModel.\n",
        "        :param x: input of shape [batch_size, slate_length, self.layers[0].in_features]\n",
        "        :return: output of shape [batch_size, slate_length, self.output_size]\n",
        "        \"\"\"\n",
        "        x = self.input_norm(x)\n",
        "        for layer in self.layers:\n",
        "            x = self.dropout(self.activation(layer(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class LTRModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents a full neural Learning to Rank model with a given encoder model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_layer, encoder, output_layer):\n",
        "        \"\"\"\n",
        "        :param input_layer: the input block (e.g. FCModel)\n",
        "        :param encoder: the encoding block (e.g. transformer.Encoder)\n",
        "        :param output_layer: the output block (e.g. OutputLayer)\n",
        "        \"\"\"\n",
        "        super(LTRModel, self).__init__()\n",
        "        self.input_layer = input_layer if input_layer else nn.Identity()\n",
        "        self.encoder = encoder if encoder else first_arg_id\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "    def prepare_for_output(self, x, mask, indices):\n",
        "        \"\"\"\n",
        "        Forward pass through the input layer and encoder.\n",
        "        :param x: input of shape [batch_size, slate_length, input_dim]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :param indices: original item ranks used in positional encoding, shape [batch_size, slate_length]\n",
        "        :return: encoder output of shape [batch_size, slate_length, encoder_output_dim]\n",
        "        \"\"\"\n",
        "        return self.encoder(self.input_layer(x), mask, indices)\n",
        "\n",
        "    def forward(self, x, mask, indices):\n",
        "        \"\"\"\n",
        "        Forward pass through the whole LTRModel.\n",
        "        :param x: input of shape [batch_size, slate_length, input_dim]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :param indices: original item ranks used in positional encoding, shape [batch_size, slate_length]\n",
        "        :return: model output of shape [batch_size, slate_length, output_dim]\n",
        "        \"\"\"\n",
        "        return self.output_layer(self.prepare_for_output(x, mask, indices), mask)\n",
        "\n",
        "    def score(self, x, mask, indices):\n",
        "        \"\"\"\n",
        "        Forward pass through the whole LTRModel and item scoring.\n",
        "\n",
        "        Used when evaluating listwise metrics in the training loop.\n",
        "        :param x: input of shape [batch_size, slate_length, input_dim]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :param indices: original item ranks used in positional encoding, shape [batch_size, slate_length]\n",
        "        :return: scores of shape [batch_size, slate_length]\n",
        "        \"\"\"\n",
        "        return self.output_layer.score(self.prepare_for_output(x, mask, indices))\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents an output block reducing the output dimensionality to d_output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_output):\n",
        "        \"\"\"\n",
        "        :param d_model: dimensionality of the output layer input\n",
        "        :param d_output: dimensionality of the output layer output\n",
        "        :param output_activation: name of the PyTorch activation function used before scoring, e.g. Sigmoid or Tanh\n",
        "        \"\"\"\n",
        "        super(OutputLayer, self).__init__()\n",
        "        self.activation = lambda y_pred: -1 + 2 * (y_pred - torch.min(y_pred)) / (torch.max(y_pred) - torch.min(y_pred))\n",
        "        self.d_output = d_output\n",
        "        self.w_1 = nn.Linear(d_model, d_output)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the OutputLayer.\n",
        "        :param x: input of shape [batch_size, slate_length, self.d_model]\n",
        "        :return: output of shape [batch_size, slate_length, self.d_output]\n",
        "        \"\"\"\n",
        "        x = self.w_1(x).squeeze(dim=2)\n",
        "        for i in range(mask.size(0)):\n",
        "            if True in mask[i].numpy():\n",
        "                assert (np.where(mask[i].numpy() == True)[0][0] > np.where(mask[i].numpy() == False)[0][-1])\n",
        "                assert (np.all(np.diff(np.where(mask[0].numpy())) == 1))\n",
        "        return torch.vstack(\n",
        "            [torch.hstack([self.activation(x[i][mask[i] == False]), x[i][mask[i] == True]]) for i in range(x.size(0))])\n",
        "\n",
        "    def score(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the OutputLayer and item scoring by summing the individual outputs if d_output > 1.\n",
        "        :param x: input of shape [batch_size, slate_length, self.d_model]\n",
        "        :return: output of shape [batch_size, slate_length]\n",
        "        \"\"\"\n",
        "        if self.d_output > 1:\n",
        "            return self.forward(x).sum(-1)\n",
        "        else:\n",
        "            return self.forward(x)\n",
        "\n",
        "\n",
        "@attrs\n",
        "class PositionalEncoding:\n",
        "    strategy = attrib(type=str)\n",
        "    max_indices = attrib(type=int)\n",
        "\n",
        "\n",
        "def make_model(params):\n",
        "    \"\"\"\n",
        "    Helper function for instantiating LTRModel.\n",
        "    :param fc_model: FCModel used as input block\n",
        "    :param transformer: transformer Encoder used as encoder block\n",
        "    :param post_model: parameters dict for OutputModel output block (excluding d_model)\n",
        "    :param n_features: number of input features\n",
        "    :return: LTR model instance\n",
        "    \"\"\"\n",
        "    fc_model = FCModel([params.hidden_dim], params.dropout, n_features=params.input_dim)  # type: ignore\n",
        "    d_model = params.input_dim if not fc_model else fc_model.output_size\n",
        "    transformer = make_transformer(n_features=d_model, N=params.n_hidden, d_ff=params.hidden_dim,\n",
        "                                   h=params.attention_heads,\n",
        "                                   positional_encoding=PositionalEncoding(strategy=params.pos_encoding,\n",
        "                                                                          max_indices=params.slength))  # type: ignore\n",
        "    model = LTRModel(fc_model, transformer, OutputLayer(d_model, d_output=params.output_dim))\n",
        "\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "\n",
        "\n",
        "class FixedPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Class implementing fixed positional encodings.\n",
        "\n",
        "    Fixed positional encodings up to max_len position are computed once during object construction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, max_len=5000):\n",
        "        \"\"\"\n",
        "        :param d_model: dimensionality of the embeddings\n",
        "        :param max_len: maximum length of the sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = torch.cat((pe, torch.zeros([1, d_model])))\n",
        "        self.padding_idx = pe.size()[0] - 1\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x, mask, indices):\n",
        "        \"\"\"\n",
        "        Forward pass through the FixedPositionalEncoding.\n",
        "        :param x: input of shape [batch_size, slate_length, d_model]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :param indices: original item ranks used in positional encoding, shape [batch_size, slate_length]\n",
        "        :return: output of shape [batch_size, slate_length, d_model]\n",
        "        \"\"\"\n",
        "        padded_indices = indices.masked_fill(mask, self.padding_idx)\n",
        "        padded_indices[padded_indices > self.padding_idx] = self.padding_idx\n",
        "        x = math.sqrt(self.pe.shape[1]) * x + self.pe[padded_indices, :]  # type: ignore\n",
        "        return x\n",
        "\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Class implementing learnable positional encodings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        :param d_model: dimensionality of the embeddings\n",
        "        :param max_len: maximum length of the sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.pe = nn.Embedding(max_len + 1, d_model, padding_idx=-1)\n",
        "\n",
        "    def forward(self, x, mask, indices):\n",
        "        \"\"\"\n",
        "        Forward pass through the LearnedPositionalEncoding.\n",
        "        :param x: input of shape [batch_size, slate_length, d_model]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :param indices: original item ranks used in positional encoding, shape [batch_size, slate_length]\n",
        "        :return: output of shape [batch_size, slate_length, d_model]\n",
        "        \"\"\"\n",
        "        padded_indices = indices.masked_fill(mask, self.pe.padding_idx)\n",
        "        padded_indices[padded_indices > self.pe.padding_idx] = self.pe.padding_idx\n",
        "        x = math.sqrt(self.pe.embedding_dim) * x + self.pe(padded_indices)\n",
        "        return x\n",
        "\n",
        "\n",
        "def _make_positional_encoding(d_model: int, positional_encoding):\n",
        "    \"\"\"\n",
        "    Helper function for instantiating positional encodings classes.\n",
        "    :param d_model: dimensionality of the embeddings\n",
        "    :param positional_encoding: config.PositionalEncoding object containing PE config\n",
        "    :return: positional encoding object of given variant\n",
        "    \"\"\"\n",
        "    if positional_encoding.strategy is None:\n",
        "        return None\n",
        "    elif positional_encoding.strategy == \"fixed\":\n",
        "        return FixedPositionalEncoding(d_model, max_len=positional_encoding.max_indices)\n",
        "    elif positional_encoding.strategy == \"learned\":\n",
        "        return LearnedPositionalEncoding(d_model, max_len=positional_encoding.max_indices)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid positional encoding type: {}\".format(positional_encoding.strategy))\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"\"\"\n",
        "    Creation of N identical layers.\n",
        "    :param module: module to clone\n",
        "    :param N: number of copies\n",
        "    :return: nn.ModuleList of module copies\n",
        "    \"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of Transformer encoder blocks with positional encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, N, position):\n",
        "        \"\"\"\n",
        "        :param layer: single building block to clone\n",
        "        :param N: number of copies\n",
        "        :param position: positional encoding module\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        self.position = position\n",
        "\n",
        "    def forward(self, x, mask, indices):\n",
        "        \"\"\"\n",
        "        Forward pass through each block of the Transformer.\n",
        "        :param x: input of shape [batch_size, slate_length, input_dim]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :param indices: original item ranks used in positional encoding, shape [batch_size, slate_length]\n",
        "        :return: output of shape [batch_size, slate_length, output_dim]\n",
        "        \"\"\"\n",
        "        if self.position:\n",
        "            x = self.position(x, mask, indices)\n",
        "        mask = mask.unsqueeze(-2)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer normalization module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        \"\"\"\n",
        "        :param features: shape of normalized features\n",
        "        :param eps: epsilon used for standard deviation\n",
        "        \"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))  # type: ignore\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))  # type: ignore\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the layer normalization.\n",
        "        :param x: input of shape [batch_size, slate_length, input_dim]\n",
        "        :return: normalized input of shape [batch_size, slate_length, output_dim]\n",
        "        \"\"\"\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual connection followed by layer normalization.\n",
        "    Please not that for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        \"\"\"\n",
        "        :param size: number of input/output features\n",
        "        :param dropout: dropout probability\n",
        "        \"\"\"\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        Forward pass through the sublayer connection module, applying the residual connection to any sublayer with the same size.\n",
        "        :param x: input of shape [batch_size, slate_length, input_dim]\n",
        "        :param sublayer: layer through which to pass the input prior to applying the sum\n",
        "        :return: output of shape [batch_size, slate_length, output_dim]\n",
        "        \"\"\"\n",
        "        return x + self.dropout(\n",
        "            sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer encoder block made of self-attention and feed-forward layers with residual connections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        \"\"\"\n",
        "        :param size: input/output size of the encoder block\n",
        "        :param self_attn: self-attention layer\n",
        "        :param feed_forward: feed-forward layer\n",
        "        :param dropout: dropout probability\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder block.\n",
        "        :param x: input of shape [batch_size, slate_length, self.size]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :return: output of shape [batch_size, slate_length, self.size]\n",
        "        \"\"\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Basic function for \"Scaled Dot Product Attention\" computation.\n",
        "    :param query: query set of shape [batch_size, slate_size, n_attention_heads, attention_dim]\n",
        "    :param key: key set of shape [batch_size, slate_size, n_attention_heads, attention_dim]\n",
        "    :param value: value set of shape [batch_size, slate_size, n_attention_heads, attention_dim]\n",
        "    :param mask: padding mask of shape [batch_size, slate_length]\n",
        "    :param dropout: dropout probability\n",
        "    :return: attention scores of shape [batch_size, slate_size, n_attention_heads, attention_dim]\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 1, float(\"-inf\"))\n",
        "\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-headed attention block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param h: number of attention heads\n",
        "        :param d_model: input/output dimensionality\n",
        "        :param dropout: dropout probability\n",
        "        \"\"\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the multi-head attention block.\n",
        "        :param query: query set of shape [batch_size, slate_size, self.d_model]\n",
        "        :param key: key set of shape [batch_size, slate_size, self.d_model]\n",
        "        :param value: value set of shape [batch_size, slate_size, self.d_model]\n",
        "        :param mask: padding mask of shape [batch_size, slate_length]\n",
        "        :return: output of shape [batch_size, slate_size, self.d_model]\n",
        "        \"\"\"\n",
        "        if mask is not None:\n",
        "            # same mask applied to all h heads\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = \\\n",
        "            [linear(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for linear, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-forward block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: input/output dimensionality\n",
        "        :param d_ff: hidden dimensionality\n",
        "        :param dropout: dropout probability\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the feed-forward block.\n",
        "        :param x: input of shape [batch_size, slate_size, self.d_model]\n",
        "        :return: output of shape [batch_size, slate_size, self.d_model]\n",
        "        \"\"\"\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "def make_transformer(N=6, d_ff=2048, h=8, dropout=0.1, n_features=136,\n",
        "                     positional_encoding=None):\n",
        "    \"\"\"\n",
        "    Helper function for instantiating Transformer-based Encoder.\n",
        "    :param N: number of Transformer blocks\n",
        "    :param d_ff: hidden dimensionality of the feed-forward layer in the Transformer block\n",
        "    :param h: number of attention heads\n",
        "    :param dropout: dropout probability\n",
        "    :param n_features: number of input/output features of the feed-forward layer\n",
        "    :param positional_encoding: config.PositionalEncoding object containing PE config\n",
        "    :return: Transformer-based Encoder with given hyperparameters\n",
        "    \"\"\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, n_features, dropout)\n",
        "\n",
        "    ff = PositionwiseFeedForward(n_features, d_ff, dropout)\n",
        "    position = _make_positional_encoding(n_features, positional_encoding)\n",
        "    return Encoder(EncoderLayer(n_features, c(attn), c(ff), dropout), N, position)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"\n",
        "    Wrapper for ndarray->Tensor conversion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "        :param sample: tuple of three ndarrays\n",
        "        :return: ndarrays converted to tensors\n",
        "        \"\"\"\n",
        "        x, y, indices = sample\n",
        "        return torch.from_numpy(x).type(torch.float32), torch.from_numpy(y).type(torch.float32), torch.from_numpy(\n",
        "            indices).type(torch.long)\n",
        "\n",
        "\n",
        "class FixLength(object):\n",
        "    \"\"\"\n",
        "    Wrapper for slate transformation to fix its length, either by zero padding or sampling.\n",
        "\n",
        "    For a given slate, if its length is less than self.dim_given, x's and y's are padded with zeros to match that length.\n",
        "    If its length is greater than self.dim_given, a random sample of items from that slate is taken to match the self.dim_given.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_given):\n",
        "        \"\"\"\n",
        "        :param dim_given: dimensionality of x after length fixing operation\n",
        "        \"\"\"\n",
        "        assert isinstance(dim_given, int)\n",
        "        self.dim_given = dim_given\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "        :param sample: ndarrays tuple containing features, labels and original ranks of shapes\n",
        "        [sample_length, features_dim], [sample_length] and [sample_length], respectively\n",
        "        :return: ndarrays tuple containing features, labels and original ranks of shapes\n",
        "            [self.dim_given, features_dim], [self.dim_given] and [self.dim_given], respectively\n",
        "        \"\"\"\n",
        "        sample_size = len(sample[1])\n",
        "        if sample_size < self.dim_given:  # when expected dimension is larger than number of observation in instance do the padding\n",
        "            fixed_len_x, fixed_len_y, indices = self._pad(sample, sample_size)\n",
        "        else:  # otherwise do the sampling\n",
        "            fixed_len_x, fixed_len_y, indices = self._sample(sample, sample_size)\n",
        "\n",
        "        return fixed_len_x, fixed_len_y, indices\n",
        "\n",
        "    def _sample(self, sample, sample_size):\n",
        "        \"\"\"\n",
        "        Sampling from a slate longer than self.dim_given.\n",
        "        :param sample: ndarrays tuple containing features, labels and original ranks of shapes\n",
        "            [sample_length, features_dim], [sample_length] and [sample_length], respectively\n",
        "        :param sample_size: target slate length\n",
        "        :return: ndarrays tuple containing features, labels and original ranks of shapes\n",
        "            [sample_size, features_dim], [sample_size] and [sample_size]\n",
        "        \"\"\"\n",
        "        indices = np.random.choice(sample_size, self.dim_given, replace=False)\n",
        "        fixed_len_y = sample[1][indices]\n",
        "        if fixed_len_y.sum() == 0:\n",
        "            if sample[1].sum() == 1:\n",
        "                indices = np.concatenate(\n",
        "                    [np.random.choice(indices, self.dim_given - 1, replace=False), [np.argmax(sample[1])]])\n",
        "                fixed_len_y = sample[1][indices]\n",
        "            elif sample[1].sum() > 0:\n",
        "                return self._sample(sample, sample_size)\n",
        "        fixed_len_x = sample[0][indices]\n",
        "        return fixed_len_x, fixed_len_y, indices\n",
        "\n",
        "    def _pad(self, sample, sample_size):\n",
        "        \"\"\"\n",
        "        Zero padding a slate shorter than self.dim_given\n",
        "        :param sample: ndarrays tuple containing features, labels and original ranks of shapes\n",
        "            [sample_length, features_dim], [sample_length] and [sample_length]\n",
        "        :param sample_size: target slate length\n",
        "        :return: ndarrays tuple containing features, labels and original ranks of shapes\n",
        "            [sample_size, features_dim], [sample_size] and [sample_size]\n",
        "        \"\"\"\n",
        "        fixed_len_x = np.pad(sample[0], ((0, self.dim_given - sample_size), (0, 0)), \"constant\")\n",
        "        fixed_len_y = np.pad(sample[1], (0, self.dim_given - sample_size), \"constant\", constant_values=PADDED_Y_VALUE)\n",
        "        indices = np.pad(np.arange(0, sample_size), (0, self.dim_given - sample_size), \"constant\",\n",
        "                         constant_values=PADDED_INDEX_VALUE)\n",
        "        return fixed_len_x, fixed_len_y, indices\n",
        "\n",
        "\n",
        "class LibSVMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    LibSVM Learning to Rank dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, query_ids, transform=None):\n",
        "        \"\"\"\n",
        "        :param X: scipy sparse matrix containing features of the dataset of shape [dataset_size, features_dim]\n",
        "        :param y: ndarray containing target labels of shape [dataset_size]\n",
        "        :param query_ids: ndarray containing group (slate) membership of dataset items of shape [dataset_size, features_dim]\n",
        "        :param transform: a callable defining an optional transformation called on the dataset\n",
        "        \"\"\"\n",
        "        # X = X.toarray()\n",
        "\n",
        "        _, indices, counts = np.unique(query_ids, return_index=True, return_counts=True)\n",
        "        groups = np.cumsum(counts[np.argsort(indices)])\n",
        "\n",
        "        self.X_by_qid = np.split(X, groups)[:-1]\n",
        "        self.y_by_qid = np.split(y, groups)[:-1]\n",
        "\n",
        "        self.longest_query_length = max([len(a) for a in self.X_by_qid])\n",
        "\n",
        "        print(\"loaded dataset with {} queries\".format(len(self.X_by_qid)))\n",
        "        print(\"longest query had {} documents\".format(self.longest_query_length))\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_env(env):\n",
        "        y = env.data[env.target].values if env.target is not None else np.zeros(len(env.data))\n",
        "        x = env.data[env.features].values\n",
        "        query_ids = env.data.index.values\n",
        "        return x, y, query_ids\n",
        "\n",
        "    @classmethod\n",
        "    def from_svm_file(cls, env, transform=None):\n",
        "        \"\"\"\n",
        "        Instantiate a LibSVMDataset from a LibSVM file path.\n",
        "        :param svm_file_path: LibSVM file path\n",
        "        :param transform: a callable defining an optional transformation called on the dataset\n",
        "        :return: LibSVMDataset instantiated from a given file and with an optional transformation defined\n",
        "        \"\"\"\n",
        "        x, y, query_ids = LibSVMDataset.load_from_env(env)  # load_svmlight_file(svm_file_path, query_id=True)\n",
        "        print(\"loaded dataset and got x shape {}, y shape {} and query_ids shape {}\".format(\n",
        "            x.shape, y.shape, query_ids.shape))\n",
        "        return cls(x, y, query_ids, transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        :return: number of groups (slates) in the dataset\n",
        "        \"\"\"\n",
        "        return len(self.X_by_qid)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        :param idx: index of a group\n",
        "        :return: ndarrays tuple containing features and labels of shapes [slate_length, features_dim] and [slate_length], respectively\n",
        "        \"\"\"\n",
        "        X = self.X_by_qid[idx]\n",
        "        y = self.y_by_qid[idx]\n",
        "\n",
        "        sample = X, y\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        \"\"\"\n",
        "        :return: shape of the dataset [batch_dim, document_dim, features_dim] where batch_dim is the number of groups\n",
        "            (slates) and document_dim is the length of the longest group\n",
        "        \"\"\"\n",
        "        batch_dim = len(self)\n",
        "        document_dim = self.longest_query_length\n",
        "        features_dim = self[0][0].shape[-1]\n",
        "        return [batch_dim, document_dim, features_dim]\n",
        "\n",
        "\n",
        "def load_libsvm_role(input_stream, role) -> LibSVMDataset:\n",
        "    \"\"\"\n",
        "    Helper function loading a LibSVMDataset of a specific role.\n",
        "\n",
        "    The file can be located either in the local filesystem or in GCS.\n",
        "    :param input_path: LibSVM file directory\n",
        "    :param role: dataset role (file name without an extension)\n",
        "    :return: LibSVMDataset from file {input_path}/{role}.txt\n",
        "    \"\"\"\n",
        "    ds = LibSVMDataset.from_svm_file(input_stream)\n",
        "    print(\"{} DS shape: {}\".format(role, ds.shape))\n",
        "    return ds\n",
        "\n",
        "\n",
        "def fix_length_to_longest_slate(ds: LibSVMDataset) -> Compose:\n",
        "    \"\"\"\n",
        "    Helper function returning a transforms.Compose object performing length fixing and tensor conversion.\n",
        "\n",
        "    Length fixing operation will fix every slate's length to maximum length present in the LibSVMDataset.\n",
        "    :param ds: LibSVMDataset to transform\n",
        "    :return: transforms.Compose object\n",
        "    \"\"\"\n",
        "    print(\"Will pad to the longest slate: {}\".format(ds.longest_query_length))\n",
        "    return transforms.Compose([FixLength(int(ds.longest_query_length)), ToTensor()])\n",
        "\n",
        "\n",
        "def load_libsvm_dataset_role(env, role: str, slate_length: int) -> LibSVMDataset:\n",
        "    \"\"\"\n",
        "    Helper function loading a single role LibSVMDataset\n",
        "    :param role: the role of the dataset - specifies file name and padding behaviour\n",
        "    :param input_path: directory containing the LibSVM files\n",
        "    :param slate_length: target slate length of the training dataset\n",
        "    :return: loaded LibSVMDataset\n",
        "    \"\"\"\n",
        "    ds = load_libsvm_role(env, role)\n",
        "    if role == \"train\":\n",
        "        ds.transform = transforms.Compose([FixLength(slate_length), ToTensor()])\n",
        "    elif role == \"val\":\n",
        "        ds.transform = fix_length_to_longest_slate(ds)\n",
        "    else:\n",
        "        ds.transform = None\n",
        "    return ds\n",
        "\n",
        "\n",
        "class MarketEnv:\n",
        "    \"\"\"\n",
        "    This class allows to represent the financial market environment\n",
        "    Prepares the target, features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            X: pd.DataFrame = None,\n",
        "            y: pd.DataFrame = None,\n",
        "            start: int = None,\n",
        "            end: int = None,\n",
        "            scaler_std_train=None,\n",
        "            scaler_mimax_train=None\n",
        "    ):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.__set_args(X.copy(), y.copy() if y is not None else y, scaler_std_train, scaler_mimax_train)\n",
        "\n",
        "    def __init_data(self, X, y):\n",
        "        \"\"\"\n",
        "        Indexing\n",
        "        \"\"\"\n",
        "        X.set_index('date', drop=True, inplace=True)\n",
        "        X = X.loc[self.start: self.end]\n",
        "        if y is not None:\n",
        "            y.set_index('date', drop=True, inplace=True)\n",
        "            y = y.loc[self.start: self.end]\n",
        "            self.data = pd.merge(y, X, on=['date', 'id'])\n",
        "            self.target = 'y'\n",
        "        else:\n",
        "            self.data = X.copy()\n",
        "            self.target = None\n",
        "        X.drop('id', axis=1, inplace=True)\n",
        "        self.features = X.columns.to_list()\n",
        "\n",
        "    def __compute_data(self, scaler_std_train, scaler_mimax_train):\n",
        "        self.n_features = len(self.features)\n",
        "        self.len_per_day = self.data.groupby('date')['0'].count()\n",
        "        self.dates = self.data.index.drop_duplicates().to_list()\n",
        "        self.data[self.features] = winsorize_zscore(self.use_scaler, scaler_std_train, self.std_scaler,\n",
        "                                                    self.data[self.features])\n",
        "        self.data[self.features] = self.use_scaler(scaler_mimax_train, self.mimax_scaler, self.data[self.features])\n",
        "\n",
        "    def __init_scaler(self, type: str, scaler_train, ticker: str = None) -> Union[MinMaxScaler, StandardScaler]:\n",
        "        \"\"\"\n",
        "        Type of scaler wanted by the user\n",
        "        For the validation and testing, the training scaler will be provided\n",
        "        \"\"\"\n",
        "        if scaler_train is None:\n",
        "            if type == 'normalized':\n",
        "                return MinMaxScaler(feature_range=(-1, 1))\n",
        "            elif type == 'standardized':\n",
        "                return StandardScaler()\n",
        "            else:\n",
        "                raise ValueError('type of scaling must be normalized or standardized')\n",
        "        else:\n",
        "            if isinstance(scaler_train, dict):\n",
        "                return scaler_train[ticker]\n",
        "            return scaler_train\n",
        "\n",
        "    @staticmethod\n",
        "    def use_scaler(scaler_train, scaler, X):\n",
        "        if scaler_train is None:  # for training sample\n",
        "            X = scaler.fit_transform(X)\n",
        "        else:  # for validation and testing sample\n",
        "            X = scaler.transform(X)\n",
        "        return X\n",
        "\n",
        "    def __set_args(self, X, y, scaler_std_train=None, scaler_mimax_train=None):\n",
        "        self.std_scaler = self.__init_scaler('standardized', scaler_std_train)\n",
        "        self.mimax_scaler = self.__init_scaler('normalized', scaler_mimax_train)\n",
        "        self.__init_data(X, y)\n",
        "        self.__compute_data(scaler_std_train, scaler_mimax_train)\n",
        "\n",
        "    def __check_params(self):\n",
        "        assert type(self.start) is type(self.end), \"Start end End data must be same type\"\n",
        "        if self.start is not None:\n",
        "            assert self.start <= self.end, \"Start of trading Nonsense\"\n",
        "\n",
        "\n",
        "class DataModule(LightningDataModule):\n",
        "    def __init__(self,\n",
        "                 train_env: MarketEnv = None,\n",
        "                 val_env: MarketEnv = None,\n",
        "                 test_env: MarketEnv = None,\n",
        "                 batch_size: int = None,\n",
        "                 slength: int = None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['train_env', 'val_env', 'test_env'])\n",
        "        self.train_env = train_env\n",
        "        self.val_env = val_env\n",
        "        self.test_env = test_env\n",
        "        self.batch_size = batch_size\n",
        "        self.n_features = self.train_env.n_features if self.train_env is not None else self.test_env.n_features\n",
        "        self.slength = slength\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_env = load_libsvm_dataset_role(self.train_env, 'train', self.slength)\n",
        "            if self.val_env is not None: self.val_env = load_libsvm_dataset_role(self.val_env, 'val', self.slength)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"predict\" or stage == \"test\" or stage is None:\n",
        "            self.test_env = load_libsvm_dataset_role(self.test_env, 'test', self.slength)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_env, shuffle=True, batch_size=self.batch_size, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        if self.val_env is not None:\n",
        "            return DataLoader(self.val_env, shuffle=False, batch_size=self.batch_size, num_workers=2)\n",
        "        return []\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.test_env, shuffle=False, batch_size=1, num_workers=2)\n",
        "\n",
        "\n",
        "def split_dates(data, end_train_date: datetime = None, split: float = None, gap_days: int = 0):\n",
        "    if split is not None: split = 1 - split\n",
        "    start_train_date = data.index[0]\n",
        "    end_test_date = data.index[-1]\n",
        "    if end_train_date is None:\n",
        "        end_train_date = (start_train_date + (end_test_date - start_train_date) * split).replace(hour=0, minute=0,\n",
        "                                                                                                 second=0,\n",
        "                                                                                                 microsecond=0,\n",
        "                                                                                                 nanosecond=0)\n",
        "    else:\n",
        "        end_train_date = to_datetime(end_train_date)\n",
        "    start_test_date = end_train_date + timedelta(days=(gap_days + 1))\n",
        "    return Set(idx=1, start=start_train_date, end=end_train_date), Set(idx=1, start=start_test_date, end=end_test_date)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Set:\n",
        "    idx: int\n",
        "    start: datetime\n",
        "    end: datetime\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Walk:\n",
        "    train: Set\n",
        "    valid: Set\n",
        "    test: Set\n",
        "\n",
        "\n",
        "def from_businessdays(bd):\n",
        "    return bd + np.ceil(bd / 5) * 2\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Set:\n",
        "    idx: int\n",
        "    start: datetime\n",
        "    end: datetime\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Walk:\n",
        "    train: Set\n",
        "    valid: Set\n",
        "\n",
        "\n",
        "class WalkForward:\n",
        "    def __init__(self,\n",
        "                 data: pd.DataFrame,\n",
        "                 val_size: float = 0,\n",
        "                 n_walks: int = 5,\n",
        "                 gap_days: int = 0,\n",
        "                 blocked: bool = False,\n",
        "                 n_steps: int = 0,\n",
        "                 max_train_size: int = None,\n",
        "                 test_size: int = None,\n",
        "                 ):\n",
        "\n",
        "        self.dates = data.date.drop_duplicates().to_list()\n",
        "        self.n_walks = n_walks\n",
        "        self.gap_days = gap_days\n",
        "        self.blocked = blocked\n",
        "        self.init(val_size, n_steps, max_train_size, test_size)\n",
        "\n",
        "    def init(self, val_size, n_steps, max_train_size, test_size):\n",
        "        if self.blocked:\n",
        "            self.cv = None\n",
        "        else:\n",
        "            self.cv = LongWalkTimeSeriesSplit(self.n_walks, 1 - val_size, self.gap_days, n_steps)\n",
        "        for idx, walk in self.get_walks(True):\n",
        "            pass\n",
        "        print(f'Samples of not used data: {self.cv.untouched_end - self.cv.untouched_start}')\n",
        "\n",
        "    def get_walks(self, verbose: bool = False):\n",
        "        idx = 0\n",
        "        for train_index, valid_index in self.cv.split(self.dates):\n",
        "            idx += 1\n",
        "            start_train = self.dates[train_index[0]]\n",
        "            end_train = self.dates[train_index[-1]]\n",
        "            start_valid = self.dates[valid_index[0]]\n",
        "            end_valid = self.dates[valid_index[-1]]\n",
        "            walk = Walk(train=Set(idx=idx, start=start_train, end=end_train),\n",
        "                        valid=Set(idx=idx, start=start_valid, end=end_valid))\n",
        "            if verbose:\n",
        "                print('*' * 20, f'{idx}th walking forward', '*' * 20)\n",
        "                print(f'Training: {walk.train.start} to {walk.train.end}')\n",
        "                print(f'Validation: {walk.valid.start} to {walk.valid.end}')\n",
        "            yield idx, walk\n",
        "\n",
        "\n",
        "class LongWalkTimeSeriesSplit:\n",
        "    def __init__(self, n_splits: int = 5, train_size: float = 0.8, gap: int = 0, time_steps: int = 0):\n",
        "        self.n_splits = n_splits\n",
        "        self.train_size = train_size\n",
        "        self.gap = gap\n",
        "        self.time_steps = time_steps\n",
        "        self.untouched_start = None\n",
        "        self.untouched_end = None\n",
        "\n",
        "    @staticmethod\n",
        "    def to_solve(train_samples, X, val_ratio, n_walks):\n",
        "        n_samples = len(X)\n",
        "        return train_samples + val_ratio * train_samples * n_walks - n_samples\n",
        "\n",
        "    def init(self, X):\n",
        "        self.train_samples = int(fsolve(self.to_solve, 1,\n",
        "                                        args=(X, 1 - self.train_size, self.n_splits))[0])\n",
        "        self.val_samples = int((1 - self.train_size) * self.train_samples)\n",
        "        assert 0.5 < self.train_size < 1, f\"Error train_size={self.train_size}\"\n",
        "\n",
        "    def split(self, X):\n",
        "        self.init(X)\n",
        "        for i in range(self.n_splits):\n",
        "            start = self.val_samples * i\n",
        "            stop = self.train_samples + self.val_samples * (i + 1)\n",
        "            mid = stop - self.val_samples + 1\n",
        "            if i == (self.n_splits - 1):\n",
        "                untouched = len(X) - stop\n",
        "                if untouched > self.gap:\n",
        "                    self.untouched_start = X[len(X) - untouched + self.gap]\n",
        "                else:\n",
        "                    self.untouched_start = X[len(X) - untouched]\n",
        "                self.untouched_end = X[len(X) - 1]\n",
        "            print(f'train: {(start, mid - self.gap)}, val: {(mid, stop)}')\n",
        "            if i == 0: print(\n",
        "                f'Due to {self.time_steps} time steps required the validation ratio is truly equal to'\n",
        "                f' {(stop - self.time_steps - mid) / (mid - self.gap - self.time_steps - start)} ')\n",
        "            yield (start, mid - self.gap), (mid, stop)\n",
        "\n",
        "\n",
        "def instantiate_2sets(data: pd.DataFrame, end_train_date: datetime = None,\n",
        "                      split: float = None,\n",
        "                      gap_days: int = 0):\n",
        "    return split_dates(data, end_train_date, split, gap_days)\n",
        "\n",
        "\n",
        "def dissociate_envs(envs, params):\n",
        "    train_env, val_env = envs\n",
        "    fit_env = (train_env, val_env)\n",
        "    params.n_envs = 3\n",
        "    return fit_env, None, params\n",
        "\n",
        "\n",
        "def compute_envs_fit(X_train, y_train, val_start: int = None, val_end: int = None,\n",
        "                     gap: int = None, model_directory_path: str = \"resources\"):\n",
        "    name = f'{model_directory_path}/scalers.pkl'\n",
        "    train_start = 0\n",
        "    train_end = val_start - gap\n",
        "    train_env = MarketEnv(X_train, y_train, train_start, train_end)\n",
        "    scaler_std_train, scaler_mimax_train = train_env.std_scaler, train_env.mimax_scaler\n",
        "    val_env = MarketEnv(X_train, y_train, val_start, val_end, scaler_std_train, scaler_mimax_train)\n",
        "    scaler = (scaler_std_train, scaler_mimax_train)\n",
        "    dump_object(scaler, name)\n",
        "    envs = (train_env, val_env)\n",
        "    return envs\n",
        "\n",
        "\n",
        "def compute_env_pred(X_test: pd.DataFrame = None, model_directory_path: str = \"resources\"):\n",
        "    name = f'{model_directory_path}/scalers.pkl'\n",
        "    scalers = load_object(name)\n",
        "    scaler_std_train, scaler_mimax_train = scalers\n",
        "    test_env = MarketEnv(X_test, None, None, None, scaler_std_train, scaler_mimax_train)\n",
        "    return test_env\n",
        "\n",
        "\n",
        "def winsorize_zscore(fun, scaler, std_scaler: StandardScaler, df: pd.DataFrame):\n",
        "    zscore = fun(scaler, std_scaler, df)\n",
        "    zscore = pd.DataFrame(zscore, index=df.index, columns=df.columns)\n",
        "    mask = np.abs(zscore) < 3\n",
        "    max_vals = df[mask].max()\n",
        "    min_vals = df[mask].min()\n",
        "    df = df.apply(lambda x: np.where(zscore[x.name] > 3, max_vals[x.name], x))\n",
        "    df = df.apply(lambda x: np.where(zscore[x.name] < -3, min_vals[x.name], x))\n",
        "    return df\n",
        "\n",
        "\n",
        "def min_max_transform(data, feature_range=(-1, 1), val_range=list()):\n",
        "    if len(val_range) == 0:\n",
        "        if not isinstance(data, torch.Tensor):\n",
        "            min_val = np.min(data)\n",
        "            max_val = np.max(data)\n",
        "        else:\n",
        "            min_val = torch.min(data)\n",
        "            max_val = torch.max(data)\n",
        "    else:\n",
        "        min_val = val_range[0]\n",
        "        max_val = val_range[-1]\n",
        "    scaled_data = ((data - min_val) / (max_val - min_val)) * (feature_range[1] - feature_range[0]) + feature_range[0]\n",
        "    return scaled_data\n",
        "\n",
        "\n",
        "def dump_object(obj, file_path):\n",
        "    os.makedirs('/'.join(file_path.split('/')[:-1]), exist_ok=True)\n",
        "    with open(file_path, 'wb') as file:\n",
        "        pickle.dump(obj, file)\n",
        "\n",
        "\n",
        "def load_object(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        obj = pickle.load(file)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def del_file(file_path):\n",
        "    open(file_path, 'w').close()\n",
        "    os.remove(file_path)\n",
        "\n",
        "def spearman_correlation_loss(y_pred, y_true):\n",
        "    y_pred = y_pred.clone()\n",
        "    y_true = y_true.clone()\n",
        "\n",
        "    mask = y_true == PADDED_Y_VALUE\n",
        "    y_pred[mask] = PADDED_Y_VALUE\n",
        "    # Calculate the mean of the target and prediction\n",
        "    mean_true = torch.mean(y_true, dim=1)\n",
        "    mean_pred = torch.mean(y_pred, dim=1)\n",
        "    # Calculate the centered values\n",
        "    centered_true = y_true - mean_true.unsqueeze(1).expand(-1, y_true.size(1))\n",
        "    centered_pred = y_pred - mean_pred.unsqueeze(1).expand(-1, y_true.size(1))\n",
        "    # Calculate the variance of th target and prediction\n",
        "    true_sqsum = torch.sum(torch.square(centered_true), dim=1)\n",
        "    pred_sqsum = torch.sum(torch.square(centered_pred), dim=1)\n",
        "    # Calculate the Pearson correlation coefficient\n",
        "    cov = torch.sum(centered_pred * centered_true, dim=1)\n",
        "    corr = cov / torch.sqrt(pred_sqsum * true_sqsum)\n",
        "    # Calculate the Squared Difference Penalty:\n",
        "    n = torch.tensor(y_pred.shape[-2], dtype=y_pred.dtype)\n",
        "    sqdif = torch.sum((y_pred - y_true) ** 2, dim=1) / n / torch.sqrt(true_sqsum / n)\n",
        "    # Estimate the Spearman Rank Correlation Loss:\n",
        "    spearman_corr = torch.tensor(1.0, dtype=y_pred.dtype) - corr + (0.01 * sqdif)\n",
        "    return torch.mean(spearman_corr, dtype=torch.float32)\n",
        "\n",
        "@dataclass\n",
        "class Params:\n",
        "    \"\"\"\n",
        "    This class allows to define the parameters of the architecture and training of a neural nets\n",
        "    \"\"\"\n",
        "    input_dim: int = 2\n",
        "    hidden_dim: int = 32\n",
        "    n_hidden: int = 1\n",
        "    dropout: float = 0.1\n",
        "    seed: int = 42\n",
        "    output_dim: int = 1\n",
        "    lr: float = 1e-3\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 100\n",
        "    l2reg: float = None\n",
        "    l1reg: float = None\n",
        "    opt: str = 'adam'\n",
        "    criterion = spearman_correlation_loss\n",
        "    attention_heads: int = 0\n",
        "    gradient_clip_val: float = None\n",
        "    n_envs: int = 2\n",
        "    slength: int = 800\n",
        "    pos_encoding: str = None\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self, monitor: list = None):\n",
        "        super().__init__()\n",
        "        self.metrics = dict(map(lambda metric: (metric, []), monitor))\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        metric = copy.deepcopy(trainer.callback_metrics)\n",
        "        for key in self.metrics.keys():\n",
        "            metric_value = metric[key].cpu().detach().numpy() if key in metric.keys() else np.nan\n",
        "            self.metrics[key].append(metric_value)\n",
        "\n",
        "\n",
        "def init_name_vparams(params, hp: list = None):\n",
        "    sorted_params = {key: value for key, value in sorted(params.__dict__.items())}\n",
        "    sorted_params = {k: v for k, v in sorted_params.items() if v is not None}\n",
        "    if hp is None:\n",
        "        name = \"__\".join([str(k) + '_' + str(round(v, 4)) for k, v in sorted_params.items() if\n",
        "                          not k in ['input_dim', 'seed', 'output_dim', 'epochs', 'opt', 'fold', 'n_steps', 'criterion',\n",
        "                                    'target']])\n",
        "    else:\n",
        "        name = \"__\".join([str(k) + '_' + str(round(v, 4)) for k, v in sorted_params.items() if k in hp])\n",
        "    return name\n",
        "\n",
        "\n",
        "def init_name_vconfig(config):\n",
        "    sorted_config = {key: value for key, value in sorted(config.items())}\n",
        "    name = \"__\".join(\n",
        "        [str(k) + '_' + str(round(v, 4)) for k, v in sorted_config.items() if not k in ['params', 'verbose_shape']])\n",
        "    return name\n",
        "\n",
        "\n",
        "def init_envs(envs, params):\n",
        "    if isinstance(envs, tuple):\n",
        "        train_env, valid_env = envs\n",
        "        params.n_envs = 3\n",
        "    else:\n",
        "        train_env = envs\n",
        "        valid_env = None\n",
        "        params.n_envs = 2\n",
        "    return train_env, valid_env, params\n",
        "\n",
        "\n",
        "def init_params(config: dict = {}, params: Params = None, index: str = None):\n",
        "    if params is None: raise Exception('Should provide a Params')\n",
        "    params = copy.deepcopy(params)\n",
        "    if index is None:\n",
        "        for key, value in config.items():\n",
        "            params.__dict__[key] = value\n",
        "    else:\n",
        "        for key in config.keys():\n",
        "            params.__dict__[key] = config[key][index]\n",
        "    return params\n",
        "\n",
        "\n",
        "def re_init_params(params: Params = None, dm: DataModule = None):\n",
        "    params.input_dim = dm.n_features\n",
        "    params.slength = dm.slength\n",
        "    return params\n",
        "\n",
        "\n",
        "\n",
        "def init_args(model=None, config: dict = {}, envs: tuple = None, patience: int = 10, metric: str = 'val_acc',\n",
        "              mode: str = 'max', params: Params = None, project: str = None, hp: list = None,\n",
        "              model_directory_path: str = \"resources\"):\n",
        "    if config is not None:\n",
        "        config = config._items\n",
        "        for key in config.copy():\n",
        "            if key not in hp:\n",
        "                config.pop(key)\n",
        "        name = init_name_vconfig(config)\n",
        "        params = init_params(config, params)\n",
        "    else:\n",
        "        name = init_name_vparams(params, hp)\n",
        "    learn_env, valid_env, params = init_envs(envs, params)\n",
        "    slength = int(learn_env.len_per_day.quantile(0.95))\n",
        "    dm = DataModule(learn_env, valid_env, None, batch_size=params.batch_size, slength=slength)\n",
        "    params = re_init_params(params, dm)\n",
        "    early_stop_callback = EarlyStopping(monitor=metric,\n",
        "                                        min_delta=0.002,\n",
        "                                        patience=patience,\n",
        "                                        verbose=True,\n",
        "                                        mode=mode,\n",
        "                                        )\n",
        "    checkpoint_callback = ModelCheckpoint(monitor=metric,\n",
        "                                          save_top_k=2,\n",
        "                                          save_last=True,\n",
        "                                          verbose=True,\n",
        "                                          dirpath=f'./{model_directory_path}',\n",
        "                                          filename='{epoch:02d}-{val_loss:.4f}-{' + metric + ':.4f}',\n",
        "                                          mode=mode)\n",
        "    metrics_callback = MetricsCallback(monitor=[metric.replace('val', 'train'), metric])\n",
        "    if isinstance(model, type): model = model(params, learn_env.n_features)\n",
        "    return model, dm, params, early_stop_callback, checkpoint_callback, metrics_callback, name\n",
        "\n",
        "\n",
        "def init_trainer(model=None, config: dict = {}, envs: tuple = None, params: Params = None, wandb_b: bool = True,\n",
        "                 patience: int = 10,\n",
        "                 name_project: str = 'MC_skew', metric: str = 'val_acc', mode: str = 'max', tuning: bool = False,\n",
        "                 hp: list = None, fast_dev_run: bool = False, model_directory_path: str = 'resources'):\n",
        "    model, dm, params, early_stop_callback, checkpoint_callback, metrics_callback, name = init_args(model, config, envs,\n",
        "                                                                                                    patience, metric,\n",
        "                                                                                                    mode,\n",
        "                                                                                                    params,\n",
        "                                                                                                    name_project, hp,\n",
        "                                                                                                    model_directory_path)\n",
        "    logger = False\n",
        "    trainer = Trainer(max_epochs=params.epochs, log_every_n_steps=10,\n",
        "                      callbacks=[early_stop_callback, checkpoint_callback, metrics_callback],\n",
        "                      logger=logger, gradient_clip_val=params.gradient_clip_val, fast_dev_run=fast_dev_run)\n",
        "    return trainer, model, dm, params, name\n",
        "\n",
        "\n",
        "def fit(model=None, config: dict = None, envs: tuple = None, params: Params = None, wandb_b: bool = True,\n",
        "        patience: int = 10,\n",
        "        project: str = 'MC_skew', metric: str = 'val_acc', mode: str = 'max', hp: list = [],\n",
        "        fast_dev_run: bool = False, model_directory_path: str = 'resources'):\n",
        "    trainer, model, dm, _, name = init_trainer(model, config, envs, params, wandb_b, patience, project, metric, mode,\n",
        "                                               hp=hp, fast_dev_run=fast_dev_run,\n",
        "                                               model_directory_path=model_directory_path)\n",
        "    trainer.fit(model, dm)\n",
        "    return trainer\n",
        "\n",
        "\n",
        "class Base:\n",
        "    def __init__(self):\n",
        "        self.device = None\n",
        "        self.model = None\n",
        "\n",
        "    def load_ckpt(self, project_name: str = None, params: Params = None, metric: str = 'val_acc',\n",
        "                  mode: str = 'max', hp: list = None, model_directory_path: str = None):\n",
        "        if params is not None:\n",
        "            ascending = False if mode == 'max' else True\n",
        "            is_saved = self._load_args(None, params, project_name, metric, ascending, hp, model_directory_path)\n",
        "        return is_saved\n",
        "\n",
        "    def _load_args(self, path: str = None, params: Params = None, project_name: str = None, metric: str = 'val_acc',\n",
        "                   ascending: bool = False, hp: list = None, model_directory_path: str = None):\n",
        "        if params is not None:\n",
        "            path = f'{model_directory_path}/'\n",
        "        check_exist = self._select_best_model(path, metric, ascending)\n",
        "        if check_exist is False:\n",
        "            return False\n",
        "        else:\n",
        "            path += self._select_best_model(path, metric, ascending)\n",
        "            if self.device.type == 'cpu':\n",
        "                checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "            else:\n",
        "                checkpoint = torch.load(path)\n",
        "            try:\n",
        "                self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            except:\n",
        "                self.load_state_dict(checkpoint['state_dict'])\n",
        "            return True\n",
        "\n",
        "    def _select_best_model(self, path: str = None, metric: str = 'val_acc', ascending: bool = False):\n",
        "        if os.path.exists(path) and not '.ckpt' in path:\n",
        "            self.delete_files(path, f'-v.ckpt')\n",
        "            for i in range(1, 5):\n",
        "                self.delete_files(path, f'-v{i}.ckpt')\n",
        "            paths = os.listdir(path)\n",
        "            if 'last.ckpt' in paths:\n",
        "                paths.remove('last.ckpt')\n",
        "                paths.remove('scalers.pkl')\n",
        "                best_path = None\n",
        "                best_metric = 1e10 if ascending else -1e10\n",
        "                for path in paths:\n",
        "                    epoch = int(path.split('epoch=')[1].split('-')[0])\n",
        "                    metric_val = float(path.split(f'{metric}=')[1].split('ckpt')[0][:-1])\n",
        "                    if epoch >= 1:\n",
        "                        if not ascending:\n",
        "                            if metric_val > best_metric:\n",
        "                                best_metric = metric_val\n",
        "                                best_path = path\n",
        "                        else:\n",
        "                            if metric_val < best_metric:\n",
        "                                best_metric = metric_val\n",
        "                                best_path = path\n",
        "                print(f'Loading best model whose {metric}={best_metric}')\n",
        "                return best_path if not best_path is None else 'last.ckpt'\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def delete_files(self, path, arg: str = ''):\n",
        "        args_file = os.listdir(path)\n",
        "        for file in args_file:\n",
        "            if arg in file:\n",
        "                delete_filename = os.path.join(path, file)\n",
        "                del_file(delete_filename)\n",
        "\n",
        "\n",
        "class BaseModel(LightningModule, Base):\n",
        "    def __init__(self, params: Params = None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['params'])\n",
        "        self.lr = params.lr\n",
        "        self.l2reg = params.l2reg\n",
        "        self.l1reg = params.l1reg\n",
        "        self.criterion = params.criterion\n",
        "        self.monitor = 'val_loss' if params.n_envs == 3 else 'train_loss'\n",
        "        seed_everything(params.seed)\n",
        "\n",
        "    def forward(self, x, mask, indices):\n",
        "        return self.model.forward(x, mask, indices)\n",
        "\n",
        "    def compute_metrics(self, pred, true, indices, type_env, loss):\n",
        "        pred = self.compute_prediction(pred)\n",
        "        mask = true == PADDED_Y_VALUE\n",
        "        pred[mask] = PADDED_Y_VALUE\n",
        "        metrics = {}\n",
        "        for name_metric, metric in self.metrics.items():\n",
        "            metric.to(self.device)\n",
        "            if not 'ndcg' in name_metric:\n",
        "                metrics[f'{type_env}_{name_metric}'] = torch.mean(torch.tensor([metric(pred[i], true[i])\n",
        "                                                                                for i in range(pred.size(0))]))\n",
        "            else:\n",
        "                metrics[f'{type_env}_{name_metric}'] = torch.mean(\n",
        "                    torch.tensor([metric(pred[i], true[i], indices[i]) for i in range(pred.size(0))]))\n",
        "        metrics[f'{type_env}_loss'] = loss\n",
        "        return metrics\n",
        "\n",
        "    def compute_l1(self):\n",
        "        l1_regularization = torch.tensor(0., device=self.device)\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                l1_regularization += torch.norm(param, p=1)\n",
        "        return self.l1reg * l1_regularization\n",
        "\n",
        "    def step(self, batch, batch_idx, type_env):\n",
        "        if type_env != 'test':\n",
        "            X, y, indices = batch\n",
        "        else:\n",
        "            X, y = batch\n",
        "            indices = torch.ones_like(y).type(torch.long)\n",
        "        mask = (y == PADDED_Y_VALUE)\n",
        "        outputs = self(X, mask, indices)\n",
        "        loss = self.criterion(outputs, y)\n",
        "        if self.l1reg is not None: loss += self.compute_l1()\n",
        "        metrics = self.compute_metrics(outputs, y, indices, type_env, loss)\n",
        "        if type_env != 'test':\n",
        "            self.log_dict(metrics, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        else:\n",
        "            for key in self.test_metrics.keys():\n",
        "                self.test_metrics[key].append(metrics[key].cpu().numpy())\n",
        "            _, indices = outputs.sort(descending=True, dim=-1)\n",
        "            self.indices.append(indices.cpu().squeeze().numpy())\n",
        "        return outputs, loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        _, loss = self.step(batch, batch_idx, type_env='train')\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # this is the validation loop\n",
        "        _, loss = self.step(batch, batch_idx, type_env='val')\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        outputs, loss = self.step(batch, batch_idx, type_env='test')\n",
        "        return self.compute_prediction(outputs).cpu().numpy()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.predict_step(batch, batch_idx)\n",
        "\n",
        "    def init_optimizer(self, opt: str):\n",
        "        if opt == 'adam':\n",
        "            return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        elif opt == 'sgd':\n",
        "            return torch.optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.l2reg)\n",
        "        elif opt == 'ranger':\n",
        "            pass\n",
        "            # return Ranger21(self.parameters(), lr=self.lr, weight_decay=self.l2reg)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # self.scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
        "        scheduler_config = {\n",
        "            \"scheduler\": ReduceLROnPlateau(\n",
        "                self.optimizer,\n",
        "                mode=\"min\",\n",
        "                factor=0.1,\n",
        "                patience=3,\n",
        "            ),\n",
        "            \"monitor\": self.monitor,  # Default: val_loss\n",
        "            \"interval\": \"epoch\",\n",
        "            \"frequency\": 1,\n",
        "            \"strict\": False,\n",
        "        }\n",
        "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": scheduler_config}\n",
        "\n",
        "    def get_test_metrics(self):\n",
        "        for key in self.test_metrics.keys():\n",
        "            self.test_metrics[key] = np.mean(self.test_metrics[key])\n",
        "        return self.test_metrics\n",
        "\n",
        "\n",
        "class LitLinear(BaseModel):\n",
        "    def __init__(self,\n",
        "                 params: Params = None,\n",
        "                 verbose_shape=None,\n",
        "                 ):\n",
        "        super().__init__(params=params)\n",
        "        self.model = make_model(params)\n",
        "        self.metrics = {\n",
        "            'spearmancorr': SpearmanCorrCoef(),\n",
        "            'ndcg': RetrievalNormalizedDCG()}\n",
        "        self.test_metrics = {'test_' + key: [] for key in self.metrics.keys()}\n",
        "        self.optimizer = self.init_optimizer(params.opt)\n",
        "        self.indices = []\n",
        "        # self.summary(verbose_shape)\n",
        "\n",
        "    def compute_prediction(self, pred):\n",
        "        return pred\n",
        "\n",
        "\n",
        "def load_model(num: int = 0, project: str = None):\n",
        "    path = f'./artifacts/{project}/model{num}'\n",
        "    with open(path + '/params.pkl', 'rb') as file:\n",
        "        params = pickle.load(file)\n",
        "    return path + '/model.ckpt', params\n",
        "\n",
        "\n",
        "def compute_fit(envs=None, model_base=None, project_name: str = None, metric: str = None, mode: str = None,\n",
        "                params: Params = None,\n",
        "                name_save: str = None, patience_es: int = 10, wandb_b: bool = True, hp: list = [],\n",
        "                fast_dev_run: bool = False, model_directory_path: str = None):\n",
        "    fit_env, test_env, params = dissociate_envs(envs, params)\n",
        "    params.input_dim = fit_env[0].n_features\n",
        "    model = model_base(params)\n",
        "    if not model.load_ckpt(project_name, params, metric, mode, hp, model_directory_path):\n",
        "        fit(envs=fit_env, model=model, patience=patience_es, metric=metric, mode=mode, project=project_name,\n",
        "            params=params, wandb_b=wandb_b, hp=hp, fast_dev_run=fast_dev_run, model_directory_path=model_directory_path)\n",
        "\n",
        "\n",
        "def compute_pred(test_env: MarketEnv = None, model_base=None, params: Params = None, project_name: str = None,\n",
        "                 metric: str = None, mode: str = None, hp: list = None, model_directory_path: str = None):\n",
        "    dm = DataModule(None, None, test_env)\n",
        "    params = re_init_params(params, dm)\n",
        "    model = model_base(params)\n",
        "    model.load_ckpt(project_name, params, metric, mode, hp, model_directory_path)\n",
        "    trainer = Trainer()\n",
        "    y = trainer.predict(model, dm)\n",
        "    y = np.hstack(y).squeeze()\n",
        "    return y, trainer.model.indices\n",
        "\n",
        "##############################\n",
        "############################\n",
        "\n",
        "def train(\n",
        "        X_train: pd.DataFrame,\n",
        "        y_train: pd.DataFrame,\n",
        "        model_directory_path: str = \"resources\",\n",
        "\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Do your model training here.\n",
        "    At each retrain this function will have to save an updated version of\n",
        "    the model under the model_directiory_path, as in the example below.\n",
        "    Note: You can use other serialization methods than joblib.dump(), as\n",
        "    long as it matches what reads the model in infer().\n",
        "\n",
        "    Args:\n",
        "        X_train, y_train: the data to train the model.\n",
        "        model_directory_path: the path to save your updated model\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    n_days = 1\n",
        "    freq = n_days\n",
        "    metric = 'val_spearmancorr'\n",
        "    mode = 'max'\n",
        "    model_base = LitLinear\n",
        "    project_name = f'Rank_all_spearman_correlation_loss'\n",
        "    hp = ['attention_heads', 'dropout', 'hidden_dim', 'lr', 'n_hidden', 'batch_size', 'l1reg']\n",
        "    params = Params(input_dim=None, hidden_dim=128, n_hidden=4, dropout=0.2477559240386763, seed=42, output_dim=1,\n",
        "                    lr=0.0016321503381104706, batch_size=2, epochs=10, l2reg=None, l1reg=0.0013452750614835098,\n",
        "                    opt='adam', attention_heads=4, gradient_clip_val=1, n_envs=None, slength=None, pos_encoding=None)\n",
        "    params.criterion = spearman_correlation_loss\n",
        "    patience_es = 3\n",
        "\n",
        "    cv = WalkForward(X_train, val_size=0.1, n_walks=2, gap_days=freq + 1, blocked=False, n_steps=1)\n",
        "    for idx, walk in cv.get_walks(False):\n",
        "        pass\n",
        "    val_start, val_end = cv.cv.untouched_start, cv.cv.untouched_end\n",
        "    envs = compute_envs_fit(X_train, y_train, val_start, val_end, freq + 1, model_directory_path)\n",
        "    compute_fit(envs, model_base, project_name, metric, mode, params,\n",
        "                name_save=None, patience_es=patience_es, wandb_b=False, hp=hp,\n",
        "                fast_dev_run=False, model_directory_path=model_directory_path)\n",
        "\n",
        "\n",
        "def infer(\n",
        "        X_test: pd.DataFrame,\n",
        "        model_directory_path: str = \"resources\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Do your inference here.\n",
        "    This function will load the model saved at the previous iteration and use\n",
        "    it to produce your inference on the current date.\n",
        "    It is mandatory to send your inferences with the ids so the system\n",
        "    can match it correctly.\n",
        "\n",
        "    Args:\n",
        "        model_directory_path: the path to the directory to the directory in wich we will be saving your updated model.\n",
        "        X_test: the independant  variables of the current date passed to your model.\n",
        "\n",
        "    Returns:\n",
        "        A dataframe (date, id, value) with the inferences of your model for the current date.\n",
        "    \"\"\"\n",
        "    metric = 'val_spearmancorr'\n",
        "    mode = 'max'\n",
        "    model_base = LitLinear\n",
        "    project_name = f'Rank_all_spearman_correlation_loss'\n",
        "    hp = ['attention_heads', 'dropout', 'hidden_dim', 'lr', 'n_hidden', 'batch_size', 'l1reg']\n",
        "    params = Params(input_dim=None, hidden_dim=32, n_hidden=2, dropout=0.20454214024409187, seed=42, output_dim=1,\n",
        "                    lr=0.001155923751088136, batch_size=2, epochs=6, l2reg=None, l1reg=0.003109444336410726, opt='adam',\n",
        "                    attention_heads=1, gradient_clip_val=1, n_envs=2, slength=3999, pos_encoding=None)\n",
        "    params.criterion = spearman_correlation_loss\n",
        "\n",
        "    y_test_predicted = X_test[[\"date\", \"id\"]].copy()\n",
        "    test_env = compute_env_pred(X_test, model_directory_path)\n",
        "    y_pred, _ = compute_pred(test_env, model_base, params, project_name, metric, mode, hp, model_directory_path)\n",
        "    y_test_predicted[\"value\"] = y_pred\n",
        "    return y_test_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyStcF3dRY5O"
      },
      "source": [
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XA4DXZtBOKz"
      },
      "source": [
        "# Testing your submission **locally**\n",
        "\n",
        "This function of the crunch package will run your code locally, in the same way in which the function is called in the cloud (ie: one date at a time). If it runs without problem, it is highly likely that there won't be problems when executing it on the CrunchDAO's system, on the cloud.\n",
        "\n",
        "You can setup the a retraining frequency as you which. A train frequency of 2 means that the system will retrain your model every two dates.\n",
        "\n",
        "`force_first_train=True` means that your model will be trained on the first date of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "crunch.test(force_first_train=True, train_frequency=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcxaEabsZrXm"
      },
      "source": [
        "# Now remember to download and then submit this notebook to https://adialab.crunchdao.com/submit"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QIGffwE2wBsi",
        "fJqTMeLqwBsm",
        "nXacC2GmwBso",
        "SwxiUFGAwBsq"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
